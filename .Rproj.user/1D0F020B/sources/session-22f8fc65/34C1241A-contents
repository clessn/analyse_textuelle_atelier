# Cleaning
library(readtext)
library(tidyverse)
library(tidytext)
library(textdata)
library(tm)
library(pdftools)
library(rebus)
library(stringr)
library(reshape2) # pour function acast (transformer df en matrix)
library(lubridate)
# Sentiment
library(quanteda)
# Topic modeling
library(topicmodels)
# Ploting
library(ggthemes)
library(sp)
library(RColorBrewer)
library(wordcloud)
library(viridisLite)
library(SnowballC)
library(plotrix)
library(dendextend)
library(radarchart) # pour faire des radar chart
library(htmlwidgets) # pour enregistrer des radar chart
library(webshot) # pour enregistrer des radar chart
library(treemap)
library(gridExtra)

####Charger les RDS####
## Tout charger les RDS en même temps
rds_all <- list.files( path = "_SharedFolder_roc-media-perception-bashing/Data/CleanROC", pattern = "*.rds", full.names = TRUE) %>%
  map_dfr(readRDS) 

# QC
dataQc_source <- DataframeSource(rds_all)
dataQc_corpus <- VCorpus(dataQc_source)

####Mots à enlever####
stopWords_en <-
  c(# Nom
    "nom", "people", "new", "old", "back", "way", "thing", "things", "left", "right", "mr", "ms",
    # Origines et politique
    "ontario", "ottawa", "toronto", "halifax", "montreal", "york", "united", "states",
    "vancouver", "canadian", "american", "canada", "government", "quebec", "minister",
    "federal", "province", "city", "québec", "quebecers", "calgary",
    #"quebec",
    # Marqueur de relations, déterminants
    "also", "per", "just", "like", "even", "still", "much", "since", "around", "well", "really", "might",
    "across", "whether", "least", "already",
    # Verbes
    "said", "says", "say", "will", "can", "get", "got", "found", "may", "told", "make", "made", "going",
    "take", "took", "think", "including", "want", "see", "called", "know", "known", "according",
    "ask", "asked", "put", "away", "among", "set", "show", "find", "went", "call", "come", "came",
    "need", "go",
    # Nombre et quantités
    "number", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten", "cent", "lot",
    "first", "second", "last", "end", "many", "former", "later", "next", "never", "always", "with", "without",
    "every", "several", "big", "short", "long", "little", "small", "less", "something", "somethings",
    # Temps et lieux
    "time", "times", "now", "lundi", "mardi", "mercredi", "jeudi", "vendredi", "samedi", "dimanche",
    "monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday", "gmt", "bst",
    "décembre", "janvier", "février", "mars", "avril", "mai", "juin", "juillet", "août", "septembre",
    "octobre", "novembre", "december", "january", "february", "march", "april", "may", "june",
    "july", "august", "september", "october", "november", "feb",
    "today", "yesterday", "another", "day", "days", "week", "weeks", "month", "months", "year", "years",
    "ago", "near", "far", "place", "early", "yet",
    # Relatif au journalisme et aux médias
    "media", "presse", "plus", "journal", "cbc", "devoir", "radio-canada", "agence", "qmi",
    "mediaqmi", "star", "cbc", "news", "press", "reuters", "reuter", "cp", "ap", "nouvelles",
    "published", "rights", "guardian", "copyright", "reserved", "timeupdated", "updated",
    "globe", "mail", "block", "related", "grdn", "anglais", "sun", "thesun", "newspapers",
    "limited", "washington", "post", "co", "tor", "ont", "french", "herald", "national post", "reuters" 
  )
# Autres
#"x", "h", "s", "t", "th", "à") # ajouter d'autres mots

####Fonction nettoyage####
# Créer une fonction pour rapidement nettoyer notre corpus
clean_corpusEN <- function(corpus){
  corpus <- tm::tm_map(corpus, content_transformer(tolower))
  corpus <- tm::tm_map(corpus, removeWords, stopwords("english"))
  corpus <- tm::tm_map(corpus, removeWords, words = stopWords_en)
  corpus <- tm::tm_map(corpus, removePunctuation, preserve_intra_word_dashes = T)
  #corpus <- tm::tm_map(corpus, removeNumbers)
  #corpus <- tm::tm_map(corpus, stemDocument) # stemming SEULEMENT pour le topic modeling
  #corpus <- tm_map(corpus, removeWords, words = keywords)
  #corpus <- tm_map(corpus, removeWords, words = after_job_en)
  corpus <- tm::tm_map(corpus, stripWhitespace)
  return(corpus)
}

clean_corp <- clean_corpusEN(dataQc_corpus)

dataQc_dtm <- DocumentTermMatrix(clean_corp)

# Pour changer un DTM en format tidy, on utilise tidy() du broom package.
data_n <- tidy(clean_corp) %>%
  bind_cols(rds_all) %>%
  select(-author, -datetimestamp, -description, -heading, -language, -origin, -doc_id, -id, -text...10) %>%
  rename(text = text...8) %>%
  unnest_tokens(word, text)

# Calculer le top 50
top50_words <- data_n %>%
  group_by(word) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(50)

####Hypothèse 1: analyse de ton####
# Créer une fonction pour analyser le ton des mots avec le Lexicoder Sentiment Dictionary
runDictionaryFunction <- function(corpusA, dataA, word, dfmA, dataB, dictionaryA) {
  corpusA <- corpus(dataA$word)
  dfmA    <- dfm(corpusA, dictionary = dictionaryA)
  dataB   <- convert(dfmA, to = "data.frame")
  return(dataB)
}

# Pour changer un DTM en format tidy, on utilise tidy() du broom package.
data_tidy <- tidy(clean_corp) %>%
  bind_cols(rds_all) %>%
  select(-author, -datetimestamp, -description, -heading, -language, -origin, -doc_id, -id, -text...10) %>%
  rename(text = text...8) %>%
  unnest_tokens(word, text)

# Pour obtenir des %
polarity <- data_tidy %>%
  mutate(date = as.POSIXct(date))  %>%
  group_by(date) %>%
  mutate(total_words = n())

# On utilise le lexicoder anglo du package de Quanteda, qui se nomme data_dictionary_LSD2015
data_ton <- runDictionaryFunction(dataA = polarity,
                                  word = word,
                                  dictionaryA = data_dictionary_LSD2015)







####Tokens####
## Each row of tkn is a token
tkn <- rds_all %>%
  unnest_tokens(
    input = text,
    token = "ngrams",
    n = 1,
    output = "token"
  ) %>% 
  filter(token %nin% stop_words$word)



####100 mots qui reviennent le plus####
top100_words <- tkn %>%
  count(token, sort = TRUE) %>%
  top_n(100, n)


####Corpus####
text_data <- rds_all$text

corpus <- Corpus(VectorSource(text_data))



# Appliquer la fonction



#content(clean_corp[[12]])

#rds_all$text[12]



##DTm##

#dataRoc_dtm <- DocumentTermMatrix(clean_corp)







#data_ton <- runDictionaryFunction(dataA = polarity,
#                                  word = word,
#                                  dictionaryA = data_dictionary_LSD2015)




#data_tidy <- tidy(clean_corp) %>%
#  bind_cols(rds_all) %>%
#  select(-author, -datetimestamp, -description, -heading, -language, -origin, -doc_id, -id, -text...10) %>%
#  rename(text = text...8) %>%
#  unnest_tokens(word, text)

####Graph####
#ggplot()

#text1 <- rds_all$text[30]
#rds_all$text

#clean_text <- LSDprep_contr(text1) 
#clean_text <- LSDprep_dict_punct(clean_text)
#clean_text <- remove_punctuation_from_acronyms(clean_text)
#clean_text <- remove_punctuation_from_abbreviations(clean_text)
#clean_text <- LSDprep_punctspace(clean_text)
#clean_text <- LSDprep_negation(clean_text)
#clean_text <- LSDprep_dict(clean_text)
#clean_text <- mark_proper_nouns(clean_text)
#clean_text <- gsub("\\.", "", clean_text)
#clean_text <- gsub(",", "", clean_text)
#clean_text <- gsub("'s", "", clean_text)
#clean_text <- gsub("'", "", clean_text)
#clean_text <- gsub('\"', "", clean_text)
#clean_text <- tolower(clean_text)

#clean_text <- lemmatize_words(clean_text)
#clean_text <- gsub("[^[:alpha:]]", " ", clean_text)
#clean_text <- gsub("\\s+", " ", clean_text)

#'%nin%' <- Negate('%in%')

# clean_corp <- clean_corpusEN(clean_text)
# 
# toks_news <- quanteda::tokens(clean_text, remove_punct = TRUE)
# 
# # get relevant keywords and phrases
# qc <- c("Québec", "Québécois", "Legault", "Quebecker", "Quebecer", "Quebeckers", "French", "français", "Loi 21", "Bill 21")
# 
# # only keep tokens specified above and their context of ±10 tokens
# # note: use phrase() to correctly score multi-word expressions
# toks_qc <- tokens_keep(toks_news, pattern = phrase(qc), window = 50)
# 
# data_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]
# 
# toks_qc_lsd <- tokens_lookup(toks_qc, dictionary = data_dictionary_LSD2015_pos_neg)
# ## Cleaning data, removing words, points, comas, etc.
# rds_all$text <- gsub("\\.", "", rds_all$text)
# rds_all$text <- gsub(",", "", rds_all$text)
# rds_all$text <- gsub("'s", "", rds_all$text)
# rds_all$text <- gsub("'", "", rds_all$text)
# rds_all$text <- gsub('\"', "", rds_all$text)
# rds_all$text <- tolower(rds_all$text)
# 
# rds_all$text <- lemmatize_words(rds_all$text)
# rds_all$text <- gsub("[^[:alpha:]]", " ", rds_all$text)
# rds_all$text <- gsub("\\s+", " ", rds_all$text)
# 
# '%nin%' <- Negate('%in%')